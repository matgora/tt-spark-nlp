{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/19 23:11:05 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n",
      "23/01/19 23:11:05 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/dn/6362h3w12mqfb_jh4vg90vp40000gn/T/temporary-e81f60e4-d28e-476b-b80d-73f39b8353da. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/01/19 23:11:05 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "----- streaming is running -------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/19 23:11:05 ERROR MicroBatchExecution: Query tweetquery [id = 224a57b7-1bd5-4a00-bcae-499c76637387, runId = 169a7ea9-0f67-4ce5-9015-03388e1ca286] terminated with error\n",
      "java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:615)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:563)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:459)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:236)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.TextSocketMicroBatchStream.initialize(TextSocketMicroBatchStream.scala:71)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.TextSocketMicroBatchStream.planInputPartitions(TextSocketMicroBatchStream.scala:117)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions$lzycompute(MicroBatchScanExec.scala:45)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions(MicroBatchScanExec.scala:45)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar(DataSourceV2ScanExecBase.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar$(DataSourceV2ScanExecBase.scala:141)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.supportsColumnar(MicroBatchScanExec.scala:29)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:459)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:145)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:145)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:158)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:158)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:657)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:647)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n"
     ]
    }
   ],
   "source": [
    "import pyspark # run after findspark.init() if you need it\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, split\n",
    "spark.sparkContext.stop()\n",
    "# Start up your pyspark session as always\n",
    "spark = SparkSession.builder.appName(\"TwitterStream\").getOrCreate()\n",
    "spark\n",
    "\n",
    "# read the tweet data from socket\n",
    "tweet_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 60439) \\\n",
    "    .load()\n",
    "\n",
    "# type cast the column value\n",
    "tweet_df_string = tweet_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Then split words based on space, filter out only hashtag (#) values and group them up.\n",
    "tweets_tab = tweet_df_string.withColumn('word', explode(split(col('value'), ' '))) \\\n",
    "    .groupBy('word') \\\n",
    "    .count() \\\n",
    "    .sort('count', ascending=False). \\\n",
    "    filter(col('word').contains('#'))\n",
    "\n",
    "writeTweet = tweets_tab \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"tweetquery\") \\\n",
    "    .start()\n",
    "print(\"----- streaming is running -------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "291b345f565dcb3f1f8b14889b9a7087b2ad3dc0d9a1bcdfce09e27ae8f8c377"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
